{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from unityagents import UnityEnvironment\n",
    "from udacity.ddpg_agent5 import Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    from sys import platform as _platform\n",
    "    if _platform == \"linux\" or _platform == \"linux2\":\n",
    "       # linux\n",
    "        env = UnityEnvironment(file_name=\"./Crawler_Linux/Crawler.x86_64\", no_graphics = True)\n",
    "    elif _platform == \"darwin\":\n",
    "       # MAC OS X\n",
    "       env = UnityEnvironment(file_name=\"Crawler.app\", no_graphics = True)\n",
    "    return env\n",
    "\n",
    "def welcome():\n",
    "    env = get_env()\n",
    "\n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "    # number of agents\n",
    "    num_agents = len(env_info.agents)\n",
    "\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "\n",
    "    # examine the state space\n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    print('Number of agents:', num_agents)\n",
    "    print('Size of each action:', action_size)\n",
    "    print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "\n",
    "    return env, state_size, action_size, num_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "def process_buffered_input(s):\n",
    "    return np.hstack(s)\n",
    "\n",
    "def init_buff():\n",
    "    ''' Initialize the buffer'''\n",
    "    buffer = deque(maxlen=SKIP_FRAME + 1)\n",
    "    for _ in range(SKIP_FRAME + 1):\n",
    "        buffer.append(np.zeros((num_agents, state_size)))\n",
    "    return buffer\n",
    "\n",
    "def ddpg(env, agent, n_episodes=2000, max_t=int(10000), prefix=''):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    episode_horizons = deque(maxlen=100)\n",
    "    brain_name = env.brain_names[0]\n",
    "    solved = False\n",
    "    reward_coeff = 0.01\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        n_agents = state.shape[0]\n",
    "        agent.reset()\n",
    "        score = np.zeros((n_agents, 1), dtype=np.float32)\n",
    "        done = np.ones(shape=(n_agents, 1), dtype=np.bool)\n",
    "\n",
    "        state_buffer = init_buff()\n",
    "        last_action = None\n",
    "        for t in range(max_t):\n",
    "            state_buffer.append(state)\n",
    "            if t < SKIP_FRAME:\n",
    "                action = np.random.uniform(-1, 1, size=(num_agents, action_size))\n",
    "            else:\n",
    "                if t % SKIP_FRAME == 0:\n",
    "                    action = agent.act(process_buffered_input([state_buffer[i] for i in range(SKIP_FRAME)]))\n",
    "                    last_action = action\n",
    "                else:\n",
    "                    action = last_action\n",
    "\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            reward = np.array(env_info.rewards)[..., None]\n",
    "            done = np.array(env_info.local_done)[..., None]\n",
    "            if t+1 == max_t:\n",
    "                done = np.ones_like(done, dtype = np.bool)\n",
    "\n",
    "            # agent.step(state, action, reward, next_state, done)\n",
    "            agent.step(process_buffered_input([state_buffer[i] for i in range(SKIP_FRAME)]),\n",
    "                       action, reward_coeff*reward,\n",
    "                       process_buffered_input([state_buffer[i] for i in range(1, SKIP_FRAME + 1)]),\n",
    "                       done)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if np.all(done):\n",
    "                episode_horizons.append(t)\n",
    "                break\n",
    "\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        writer.add_scalar('score_G', np.mean(scores), i_episode)\n",
    "        \n",
    "        _mu_score_moving = np.mean(np.mean(scores_deque, axis = 1))\n",
    "        print('\\rEpisode {}\\t100-episode avg score: {:.2f}\\tScore: {:.2f}\\tTime Step: {}'.format(i_episode, _mu_score_moving, float(np.mean(score)), agent.total_steps), end=\"\")\n",
    "\n",
    "        if i_episode % 50 == 0:\n",
    "            print('\\rEpisode {}\\t100-episode avg score: {:.2f}\\tAvg. Horizon: {:.2f}'.format(i_episode, _mu_score_moving, np.mean(episode_horizons)))\n",
    "\n",
    "        if (np.mean(scores_deque) >= 30.) and (i_episode > 99) and (not solved):\n",
    "            print('The environment was solved in {} episodes'.format(i_episode))\n",
    "            solved = True\n",
    "\n",
    "    torch.save(agent.actor_local.state_dict(), './models/{}checkpoint_actor_multi.pth'.format(prefix))\n",
    "    torch.save(agent.critic_local.state_dict(), './models/{}checkpoint_critic_multi.pth'.format(prefix))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 12\n",
      "Size of each action: 20\n",
      "There are 12 agents. Each observes a state with length: 129\n"
     ]
    }
   ],
   "source": [
    "SKIP_FRAME = 3\n",
    "env, state_size, action_size, num_agents = welcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG\n",
    "\n",
    "Run the code cell below to train the agent from scratch.  Alternatively, you can skip to the next code cell to load the pre-trained weights from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 1000\n",
    "n_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\t100-episode avg score: 66.71\tAvg. Horizon: 999.00tep: 500000\n",
      "Episode 100\t100-episode avg score: 192.06\tAvg. Horizon: 999.00tep: 100000\n",
      "The environment was solved in 100 episodes\n",
      "Episode 150\t100-episode avg score: 237.92\tAvg. Horizon: 999.00tep: 150000\n",
      "Episode 200\t100-episode avg score: 125.90\tAvg. Horizon: 999.00tep: 200000\n",
      "Episode 250\t100-episode avg score: 200.19\tAvg. Horizon: 999.00tep: 250000\n",
      "Episode 300\t100-episode avg score: 340.93\tAvg. Horizon: 999.00tep: 300000\n",
      "Episode 349\t100-episode avg score: nan\tScore: nan\tTime Step: 349000348000"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f6b36fe15ab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m               explore_assumptions=True)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'crawler_reflective_critic_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-c3d57b26c655>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(env, agent, n_episodes, max_t, prefix)\u001b[0m\n\u001b[1;32m     51\u001b[0m                        \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_coeff\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                        \u001b[0mprocess_buffered_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSKIP_FRAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                        done)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Classes_and_Training/Udacity/RL/deep-reinforcement-learning/p2_continuous-control/udacity/ddpg_agent5.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperturb_the_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_sample_perturbed_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_perturbation_sigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Classes_and_Training/Udacity/RL/deep-reinforcement-learning/p2_continuous-control/udacity/ddpg_agent5.py\u001b[0m in \u001b[0;36m_compute_sample_perturbed_distance\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_sample_perturbed_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperturbed_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='./logs/crawler_reflective_critic_multi/horizon_{}'.format(max_t))\n",
    "agent = Agent(state_size=state_size*SKIP_FRAME,\n",
    "              action_size=action_size,\n",
    "              random_seed = 0,\n",
    "              writer=writer,\n",
    "              explore_assumptions=True)\n",
    "\n",
    "scores = ddpg(env, agent, n_episodes=n_episodes, max_t = max_t, prefix='crawler_reflective_critic_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), [np.mean(s) for s in scores])\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
